{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Piano Music with Transformer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-RWsbrhmPpP"
      },
      "source": [
        "##### Copyright 2019 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI5g-x4foZls"
      },
      "source": [
        "# Generating Piano Music with Transformer\n",
        "### ___Ian Simon, Anna Huang, Jesse Engel, Curtis \"Fjord\" Hawthorne___\n",
        "\n",
        "This Colab notebook lets you play with pretrained [Transformer](https://arxiv.org/abs/1706.03762) models for piano music generation, based on the [Music Transformer](http://g.co/magenta/music-transformer) model introduced by [Huang et al.](https://arxiv.org/abs/1809.04281) in 2018.\n",
        "\n",
        "The models used here were trained on over 10,000 hours of piano recordings from YouTube, transcribed using [Onsets and Frames](http://g.co/magenta/onsets-frames) and represented using the event vocabulary from [Performance RNN](http://g.co/magenta/performance-rnn).\n",
        "\n",
        "Unlike the original Music Transformer paper, this notebook uses attention based on absolute instead of relative position; we may add models that use relative attention at some point in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDMQbHPYVKmV"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tciXVi5eWG_1",
        "cellView": "form"
      },
      "source": [
        "#@title Setup Environment\n",
        "#@markdown Copy some auxiliary data from Google Cloud Storage.\n",
        "#@markdown Also install and import Python dependencies needed\n",
        "#@markdown for running the Transformer models.\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "print('Copying Salamander piano SoundFont (via https://sites.google.com/site/soundfonts4u) from GCS...')\n",
        "!gsutil -q -m cp -r gs://magentadata/models/music_transformer/primers/* /content/\n",
        "!gsutil -q -m cp gs://magentadata/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /content/\n",
        "\n",
        "print('Installing dependencies...')\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 build-essential libasound2-dev libjack-dev\n",
        "!pip install -q 'tensorflow-datasets < 4.0.0'\n",
        "!pip install -qU google-cloud magenta pyfluidsynth\n",
        "\n",
        "print('Importing libraries...')\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from tensor2tensor import models\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "from tensor2tensor.utils import decoding\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "\n",
        "from magenta.models.score2perf import score2perf\n",
        "import note_seq\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URxzTQyXfdO",
        "cellView": "form"
      },
      "source": [
        "#@title Definitions\n",
        "#@markdown Define a few constants and helper functions.\n",
        "\n",
        "SF2_PATH = '/content/Yamaha-C5-Salamander-JNv5.1.sf2'\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "# Upload a MIDI file and convert to NoteSequence.\n",
        "def upload_midi():\n",
        "  data = list(files.upload().values())\n",
        "  if len(data) > 1:\n",
        "    print('Multiple files uploaded; using only one.')\n",
        "  return note_seq.midi_to_note_sequence(data[0])\n",
        "\n",
        "# Decode a list of IDs.\n",
        "def decode(ids, encoder):\n",
        "  ids = list(ids)\n",
        "  if text_encoder.EOS_ID in ids:\n",
        "    ids = ids[:ids.index(text_encoder.EOS_ID)]\n",
        "  return encoder.decode(ids)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl3oY0w8gBJh"
      },
      "source": [
        "# Piano Performance Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBngSJvP_En7",
        "cellView": "form"
      },
      "source": [
        "#@title Setup and Load Checkpoint\n",
        "#@markdown Set up generation from an unconditional Transformer\n",
        "#@markdown model.\n",
        "\n",
        "model_name = 'transformer'\n",
        "hparams_set = 'transformer_tpu'\n",
        "ckpt_path = 'gs://magentadata/models/music_transformer/checkpoints/unconditional_model_16.ckpt'\n",
        "\n",
        "class PianoPerformanceLanguageModelProblem(score2perf.Score2PerfProblem):\n",
        "  @property\n",
        "  def add_eos_symbol(self):\n",
        "    return True\n",
        "\n",
        "problem = PianoPerformanceLanguageModelProblem()\n",
        "unconditional_encoders = problem.get_feature_encoders()\n",
        "\n",
        "# Set up HParams.\n",
        "hparams = trainer_lib.create_hparams(hparams_set=hparams_set)\n",
        "trainer_lib.add_problem_hparams(hparams, problem)\n",
        "hparams.num_hidden_layers = 16\n",
        "hparams.sampling_method = 'random'\n",
        "\n",
        "# Set up decoding HParams.\n",
        "decode_hparams = decoding.decode_hparams()\n",
        "decode_hparams.alpha = 0.0\n",
        "decode_hparams.beam_size = 1\n",
        "\n",
        "# Create Estimator.\n",
        "run_config = trainer_lib.create_run_config(hparams)\n",
        "estimator = trainer_lib.create_estimator(\n",
        "    model_name, hparams, run_config,\n",
        "    decode_hparams=decode_hparams)\n",
        "\n",
        "# Create input generator (so we can adjust priming and\n",
        "# decode length on the fly).\n",
        "def input_generator():\n",
        "  global targets\n",
        "  global decode_length\n",
        "  while True:\n",
        "    yield {\n",
        "        'targets': np.array([targets], dtype=np.int32),\n",
        "        'decode_length': np.array(decode_length, dtype=np.int32)\n",
        "    }\n",
        "\n",
        "# These values will be changed by subsequent cells.\n",
        "targets = []\n",
        "decode_length = 0\n",
        "\n",
        "# Start the Estimator, loading from the specified checkpoint.\n",
        "input_fn = decoding.make_input_fn_from_generator(input_generator())\n",
        "unconditional_samples = estimator.predict(\n",
        "    input_fn, checkpoint_path=ckpt_path)\n",
        "\n",
        "# \"Burn\" one.\n",
        "_ = next(unconditional_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ybYgKSgIt-",
        "cellView": "form"
      },
      "source": [
        "#@title Generate from Scratch\n",
        "#@markdown Generate a piano performance from scratch.\n",
        "#@markdown\n",
        "#@markdown This can take a minute or so depending on the length\n",
        "#@markdown of the performance the model ends up generating.\n",
        "#@markdown Because we use a \n",
        "#@markdown [representation](http://g.co/magenta/performance-rnn)\n",
        "#@markdown where each event corresponds to a variable amount of\n",
        "#@markdown time, the actual number of seconds generated may vary.\n",
        "\n",
        "targets = []\n",
        "decode_length = 1024\n",
        "\n",
        "# Generate sample events.\n",
        "sample_ids = next(unconditional_samples)['outputs']\n",
        "\n",
        "# Decode to NoteSequence.\n",
        "midi_filename = decode(\n",
        "    sample_ids,\n",
        "    encoder=unconditional_encoders['targets'])\n",
        "unconditional_ns = note_seq.midi_file_to_note_sequence(midi_filename)\n",
        "\n",
        "# Play and plot.\n",
        "note_seq.play_sequence(\n",
        "    unconditional_ns,\n",
        "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
        "note_seq.plot_sequence(unconditional_ns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R7s3MmldBBB",
        "cellView": "form"
      },
      "source": [
        "#@title Download Performance as MIDI\n",
        "#@markdown Download generated performance as MIDI (optional).\n",
        "\n",
        "note_seq.sequence_proto_to_midi_file(\n",
        "    unconditional_ns, '/tmp/unconditional.mid')\n",
        "files.download('/tmp/unconditional.mid')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}